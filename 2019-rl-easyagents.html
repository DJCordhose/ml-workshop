<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">

    <title>Easyagents</title>

    <meta name="description" content="A framework for easily creating beautiful presentations using HTML">
    <meta name="author" content="Hakim El Hattab">

    <meta name="apple-mobile-web-app-capable" content="yes"/>
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"/>

    <meta name="viewport"
          content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

          <link rel="stylesheet" href="reveal.js/css/reveal.css">
          <!--<link rel="stylesheet" href="reveal.js/css/theme/white.css" id="theme">-->
          <!--<link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">-->
          <!--<link rel="stylesheet" href="reveal.js/css/theme/night.css" id="theme">-->
          <!--<link rel="stylesheet" href="reveal.js/css/theme/simple.css" id="theme">-->
          <link rel="stylesheet" href="reveal.js/css/theme/solarized.css" id="theme">
      
          <!-- Code syntax highlighting -->
          <link rel="stylesheet" href="reveal.js/lib/css/zenburn.css">
          <style>
              /*pre code {*/
                  /*display: block;*/
                  /*padding: 0.5em;*/
                  /*background: #FFFFFF !important;*/
                  /*color: #000000 !important;*/
              /*}*/
      
              .right-img {
                  margin-left: 10px !important;
                  float: right;
                  height: 500px;
              }
              .todo:before {
                  content: 'TODO: ';
              }
              .todo {
                  color: red !important;
              }
              code span.line-number {
                  color: lightcoral;
              }
              .reveal pre code {
                  max-height: 1000px !important;
              }
      
              img {
                  border: 0 !important;
                  box-shadow:0 0 0 0 !important;
              }
      
              .reveal {
                  -ms-touch-action: auto !important;
                  touch-action: auto !important;
                      }
      
                      .reveal h2,
                      .reveal h3,
                      .reveal h4 {
                          letter-spacing: 2px;
                          font-family: 'Amiri', serif;
                          /* font-family: 'Times New Roman', Times, serif; */
                          font-weight: bold;
                          font-style: italic;
                          letter-spacing: -2px;
                          text-transform: none !important;
                      }
      
                      .reveal em {
                          font-weight: bold;
                      }
      
                      .reveal .step-subtitle h1 {
                          letter-spacing: 1px;
                      }
                      .reveal .step-subtitle h2,
                      .reveal .step-subtitle h3 {
                          text-transform: none;
                          font-style: italic;
                          font-weight: normal;
                          /* font-weight: 400; */
                          /* font-family: 'Amiri', serif; */
                          font-family: 'Lobster', serif;
                          letter-spacing: 1px;
                          color: #2aa198;
                          text-decoration: underline;
                      }
      
                      .reveal .front-page h1,
                      .reveal .front-page h2 {
                          font-family: "League Gothic";
                          font-style: normal;
                          text-transform: uppercase !important;
                          letter-spacing: 1px;
                      }
      
                      .reveal .front-page h1 {
                          font-size: 2.5em !important;
                      }
      
                      .reveal .highlight {
                          background-color: #D3337B;
                          color: white;
                      }
      
              .reveal section img {
                background: none;
              }
      
                      .reveal img.with-border {
                          border: 1px solid #586e75 !important;
                          box-shadow: 3px 3px 1px rgba(0, 0, 0, 0.15) !important;
                      }
      
                      .reveal li {
                          margin-bottom: 8px;
                      }
      
                      /* For li's that use FontAwesome icons as bullet-point */
                  .reveal ul.fa-ul li {
                      list-style-type: none;
                  }
          </style>
      
    <!-- Printing and PDF exports -->
    <script>
        var link = document.createElement('link');
        link.rel = 'stylesheet';
        link.type = 'text/css';
        var printMode = window.location.search.match(/print-pdf/gi);
        link.href = printMode ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
        document.getElementsByTagName('head')[0].appendChild(link);
    </script>

    <!--[if lt IE 9]>
    <script src="reveal.js/lib/js/html5shiv.js"></script>
    <![endif]-->
</head>

<body>

<div class="reveal">
    <div class="slides">
<!-- 
            <section data-markdown class="preparation">
                    <textarea data-template>
### Preparation

                </textarea>
            </section>
             -->
<!-- 
            <section data-markdown class="todo">
                    <textarea data-template>
            </textarea>
            </section>    
 -->

 <section data-markdown>
        <textarea data-template>
### Alte Folien
* Wie in Göteborg gemacht (neuer): https://djcordhose.github.io/ai/2019_tf_rl.html
* Wie bei MACH gemacht (älter): https://djcordhose.github.io/ai/2019_rl.html
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
## Structure of Observation and Reward are crucial
* agents can learn short cuts and unexpected behavior
* Funny list of machines learned as opposed what the designers intended them to learn
  * `Reward-shaping a soccer robot for touching the ball caused it to learn to get to the ball and vibrate touching it as fast as possible`
  * `Robot hand pretending to grasp an object by moving between the camera and the object`
  * `Simulated pancake making robot learned to throw the pancake as high in the air as possible in order to maximize time away from the ground`
  * `Agent pauses the game indefinitely to avoid losing`
  * `In an artificial life simulation where survival required energy but giving birth had no energy cost, one species evolved a sedentary lifestyle that consisted mostly of mating in order to produce new children which could be eaten (or used as mates to produce more edible children).`
  * `Agent kills itself at the end of level 1 to avoid losing in level 2`
  * `Evolved player makes invalid moves far away in the board, causing opponent players to run out of memory and crash`
  * `Creatures exploited physics simulation bugs by twitching, which accumulated simulator errors and allowed them to travel at unrealistic speeds`
  * `Reward-shaping a bicycle agent for not falling over & making progress towards a goal point (but not punishing for moving away) leads it to learn to circle around the goal in a physically stable loop.`
  * `... algorithm learns to bait an opponent into following it off a cliff, which gives it enough points for an extra life, which it does forever in an infinite loop.`
  * `The PPO algorithm discovers that it can slip through the walls of a level to move right and attain a higher score.`

Source: https://vkrakovna.wordpress.com/2018/04/02/specification-gaming-examples-in-ai/
  * https://arxiv.org/abs/1803.03453 
  * https://docs.google.com/spreadsheets/u/1/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml
  * https://twitter.com/mogwai_poet/status/1060286856493813760
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
# How good can you get? What is the optimum?

* generally you just do not know
* the higher the score the better
* baselines can help in Reinforcement Learning as well
* in our special case, there is a deterministic baseline
  * best first search (https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm)
  * guarentees to find optimal solution
  * exponential complexity
  * but works fine for our size of the problem
  * optimal score between _.73_ and _.74_ with very low variance
  * number of steps to pass through complete turf around _17_
* our reinforcement learning approach trades the perfect solution for
  * nearly perfect solution 
  * linear time complexity
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### When RL is a promising approach

1. You have no data
1. You can simulate your environment or can make experiments in high numbers in the real world without danger (or combine both)
1. Menschen können das
   * Menschen entscheiden 'intuitiv', ie in wenigen Sekunden. Machen keine Recherche.
1. Kann der Experte wechseln während der Aufgabe?
5.Du weißt was gut und schlecht ist / kannst eine Lösung mit Punkten bewerten
6. Pluspunkt: es gibt mehr als 1 Lösung
7. Pluspunkt: du kannst auf dem weg schon sagen was tendenziell eine gute Richtung ist
1. Halbscherz: wenn wir Spiele Designer wären gäbe das ein tolles Spiel.
  * ..und wir sollten es auf die playstation bringen
</textarea>
</section>


 <section data-markdown>
        <textarea data-template>
### Data2day Talk

* 15 Minuten: Christian Intro 

* 15 Minuten: Olli Demo Notebook
  1. lokal
  1. Reset and Clear Output
  1. Von oben nach unten durchgehen

* 5 Minuten: Christian Practical

* 5 Minuten Olli: Easyagents

* 5 Minuten Fragen und Abschluss
</textarea>
</section>
     
 <section data-markdown class="todo">
    <textarea data-template>
### Understanding PPO

* https://medium.com/@jonathan_hui/rl-proximal-policy-optimization-ppo-explained-77f014ec3f12 
* https://openai.com/blog/openai-baselines-ppo/
* https://www.reddit.com/r/reinforcementlearning/comments/8l9hkk/d_what_is_the_actual_cost_function_for_ppo/  
</textarea>
</section>

 <section data-markdown>
    <textarea data-template>
### KL Divergence in PPO

_we don’t want any new policy to be too different from the current one_

* a measure of how difference of two probability distributions
    * https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence
* Penalize big changes in the action distribution from policy network 
* Adding KL Divergence as part of the loss function
* Makes it accessible to standard first order optimizers in NN backpropagation
  * https://medium.com/@jonathan_hui/rl-proximal-policy-optimization-ppo-explained-77f014ec3f12

</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### World Models

_learn an abstract representation of the world and use it for reinforcement learning to achieve a simple policy_ 

<img src='img/rl/world-models.gif' height="350">
<br>
<small>

https://worldmodels.github.io/
<br>
https://youtu.be/HzA8LRqhujk?t=256

</small>

</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### Car Racing

<video controls src="img/rl/carracing_vae_compare.mp4" 
type="video/mp4" height="400"></video>
<small>
https://worldmodels.github.io/
</small>        
        
</textarea>
</section>


 <section data-markdown>
    <textarea data-template>
### Data2day Talk
## In Folien nur Konzept zeigen, allen Code in Notebook    
    
 Orso und PPO erklären
 Alles von Policy to Observation muss weg 
 immer wieder dahin zurück kehren und zeigen wo man ist
Dann weiter mit Practiioners View
</textarea>
</section>
 


<section data-markdown>
        <textarea data-template>
## Applying Reinforcement Learning to a problem

* Is your problem approachable by Reinforcement Learning (more on that later)?
  * Can you define what would be the agent and what the environment?
* You can simulate your environment or are able to safely perform a large number of experiments in the real world
* You need to be able to define your problem as a Markov Decision Process (MDP)
* Good sanity check: Would you as a human be able to play the game based on the observation and reward you get?
* there are different kinds of observations
  * fully observed environment (e.g. Jump'n'Run Game)
  * partially observed environment (e.g. Ego Shooter)
* Choose a proper Reinforcement Learning Algorithm (more on that later)
</textarea>
</section>

<section data-markdown>
        <textarea data-template>
## MDP: Markov Decision Process in Reinforcement Learning

* https://en.wikipedia.org/wiki/Markov_decision_process
* Model your observation in a way that which actions the agent took to get to a certain state are irrelevant
* That means all information necessary to derive a proper action need to be in a single observation
* The agents must not be forced to also learn the sequence of observations
* E.g. the position of the PacMan is not enough. Which path taken results in where the ghosts are and what pills have been eaten
</textarea>
</section>

<section data-markdown style="font-size: large;">
<textarea data-template>
### Notebooks

Workshop

https://colab.research.google.com/github/christianhidber/easyagents/blob/master/jupyter_notebooks/workshop/orso.ipynb
http://bit.ly/easyagents-orso
https://colab.research.google.com/github/christianhidber/easyagents/blob/master/jupyter_notebooks/workshop/easyagents_line_exercise.ipynb
http://bit.ly/easyagents-lineworld

Talk

tf agents raw
https://colab.research.google.com/github/christianhidber/easyagents/blob/master/jupyter_notebooks/tf_agents_orso_talk.ipynb
http://bit.ly/tf-agents-orso

easyagents
https://colab.research.google.com/github/christianhidber/easyagents/blob/master/jupyter_notebooks/easyagents_orso_talk.ipynb
https://colab.research.google.com/github/christianhidber/easyagents/blob/master/jupyter_notebooks/easyagents_orso.ipynb
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Environment vs Observation

Snapshot of one Levle: Observation
https://www.mariowiki.com/images/7/78/SMAS_LL_World_4-1_Screenshot.png 

All Levels: Environment
https://i.ytimg.com/vi/de4Wd56fqTM/maxresdefault.jpg 
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
#### TF Agents Overview

<img src='img/rl/tf-agents-overview.png' height="600px"> 
</textarea>
</section>

            <section data-markdown>
                    <textarea data-template>
## Pragmatic Reinforcement Learning with Easyagent

https://djcordhose.github.io/ml-workshop/2019-rl-easyagents.html
                    </textarea>
                </section>

<section data-markdown>
        <textarea data-template>
### Find the optimal route

<img src='img/rl/turf.png' height="450px">

<small>

https://opendatascience.com/bears-need-to-learn-as-well-practical-reinforcement-learning-with-tensorflow-2-0-tf-agents/

</small>
    </textarea>
</section>

<section data-markdown>
<textarea data-template>
### Reinforcement Learning: Using an Agent

<img src='img/rl/rl-train-play-1.png' height="550">

</textarea>
</section>

<section data-markdown>
<textarea data-template>
### Reinforcement Learning: Training an agent

<img src='img/rl/rl-train-play-2.png' height="550">

</textarea>
</section>

<section data-markdown>
<textarea data-template>
### Reinforcement Learning Using Easyagents

_Main Challenge is defining your environment / simulation_
<img src='img/rl/easyagents-complex.jpg'>

<small>

https://github.com/christianhidber/easyagents

</small>

</textarea>
</section>


<section data-markdown>
        <textarea data-template>
### Solution
<video controls src="img/rl/orso-sample-path.mp4"  muted type="video/mp4" height="500"></video>
                
</textarea>
</section>

<section data-markdown>
<textarea data-template>
### Easyagents

<!-- <img src='img/rl/orso_raw.png' style="float: left;" height="300px"> -->

<!-- <div style="float: right; width: 600px; font-size: xx-large"> -->

<div>
    
* Abstraction over complex solutions
* Allows you to concentrate on defining your problem
* No need to go through the details of algorithms (we did this for you)
* Supports TF-Agents and OpenAI (more to come)
* Reinforce (Vanilla Policy Gradient), PPO, and DQN work out of the box
</div>

<small style="clear: both">

https://github.com/christianhidber/easyagents

</small>

</textarea>
</section>



    </div>

</div>

<script src="reveal.js/js/reveal.js"></script>
<script src="lib/jquery-2.2.4.js"></script>

<script>
        const isLocal = window.location.hostname.indexOf('localhost') !== -1 || 
                    window.location.hostname.indexOf('127.0.0.1') !== -1;
    
        if (isLocal && !printMode) {
        } else {
            // only applies to public version
            $('.todo').remove();
            $('.preparation').remove();
            $('.local').remove();
        }
        Reveal.addEventListener( 'ready', function( event ) {
            $('.fragments li').addClass('fragment')

            if (isLocal && !printMode) {
                // only applies to presentation version
                Reveal.configure({ controls: false });
            } else {
                // only applies to public version
                $('.fragment').removeClass('fragment');
            }
            // applies to all versions
            $('code').addClass('line-numbers');

            // make all links open in new tab
            $('a').attr('target', '_blank')

        } );
        $('section:not([data-background])').attr('data-background', "background/white.jpg");
        // $('section:not([data-background])').attr('data-background', "background/white-transparent.jpg");
        // $('section').attr('data-background-size', "1620px");

    </script>
    
<script>

    // Full list of configuration options available at:
    // https://github.com/hakimel/reveal.js#configuration
    Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        center: true,
        width: 1100,

        transition: 'fade', // none/fade/slide/convex/concave/zoom

        math: {
            mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
            config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
        },

        // Optional reveal.js plugins
        dependencies: [
            {
                src: 'reveal.js/lib/js/classList.js', condition: function () {
                return !document.body.classList;
            }
            },
            {
                src: 'reveal.js/plugin/markdown/marked.js', condition: function () {
                return !!document.querySelector('[data-markdown]');
            }
            },
            {
                src: 'reveal.js/plugin/markdown/markdown.js', condition: function () {
                return !!document.querySelector('[data-markdown]');
            }
            },
            {
                src: 'reveal.js/plugin/highlight/highlight.js', async: true, condition: function () {
                return !!document.querySelector('pre code');
            }, callback: function () {
                hljs.initHighlightingOnLoad();
            }
            },
            {src: 'reveal.js/plugin/zoom-js/zoom.js', async: true},
            {src: 'reveal.js/plugin/notes/notes.js', async: true},
            // https://github.com/mikemiles86/reveal-line-numbers
            {src: 'lib/js/line-numbers.js'},
            { src: 'reveal.js/plugin/math/math.js', async: true }
        ]
    });

</script>

</body>
</html>
