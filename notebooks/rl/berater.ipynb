{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "berater.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "zpzHtN3-kQ26",
        "w3OdHyWEEEwy",
        "bzoq0VM85p46"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DJCordhose/ml-workshop/blob/master/notebooks/rl/berater.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "eU7ylMh1kQ2y"
      },
      "cell_type": "markdown",
      "source": [
        "# Berater Environment v13\n",
        "\n",
        "## Changes from v12 (work in progress)\n",
        "* port to tfagents with ppo\n",
        "* openai implementation removed"
      ]
    },
    {
      "metadata": {
        "id": "PQiN7IVMS6SC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "berater_show_step=False \n",
        "berater_show_done=False \n",
        "berater_debug_step=False "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "zpzHtN3-kQ26"
      },
      "cell_type": "markdown",
      "source": [
        "## Install tf-agents"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "0E567zPTkQ28",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install tf-agents-nightly > /dev/null\n",
        "!pip install tf-nightly > /dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w3OdHyWEEEwy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Define Environment"
      ]
    },
    {
      "metadata": {
        "id": "sQ8Nfk3MKgLt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ]
    },
    {
      "metadata": {
        "id": "HQyb_Aq8Kg9j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import gym\n",
        "from gym.utils import seeding\n",
        "from gym import spaces\n",
        "\n",
        "import pdb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OsJ6zcXvwN53",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Helper methods"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-S4sZG5ZkQ3T",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def state_name_to_int(state):\n",
        "    state_name_map = {\n",
        "        'S': 0,\n",
        "        'A': 1,\n",
        "        'B': 2,\n",
        "        'C': 3,\n",
        "        'D': 4,\n",
        "        'E': 5,\n",
        "        'F': 6,\n",
        "        'G': 7,\n",
        "        'H': 8,\n",
        "        'K': 9,\n",
        "        'L': 10,\n",
        "        'M': 11,\n",
        "        'N': 12,\n",
        "        'O': 13\n",
        "    }\n",
        "    return state_name_map[state]\n",
        "\n",
        "def int_to_state_name(state_as_int):\n",
        "    state_map = {\n",
        "        0: 'S',\n",
        "        1: 'A',\n",
        "        2: 'B',\n",
        "        3: 'C',\n",
        "        4: 'D',\n",
        "        5: 'E',\n",
        "        6: 'F',\n",
        "        7: 'G',\n",
        "        8: 'H',\n",
        "        9: 'K',\n",
        "        10: 'L',\n",
        "        11: 'M',\n",
        "        12: 'N',\n",
        "        13: 'O'\n",
        "    }\n",
        "    return state_map[state_as_int]\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x-olom0nwiSX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Berater Environment (OpenAI Gym)"
      ]
    },
    {
      "metadata": {
        "id": "3plH2u3Swotj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BeraterEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    The Berater Problem\n",
        "\n",
        "    Actions: \n",
        "    There are 4 discrete deterministic actions, each choosing one direction\n",
        "    \"\"\"\n",
        "    metadata = {'render.modes': ['ansi']}\n",
        "    \n",
        "    showStep = False\n",
        "    showDone = True\n",
        "    envEpisodeModulo = 100\n",
        "\n",
        "    def __init__(self):\n",
        "#         self.map = {\n",
        "#             'S': [('A', 100), ('B', 400), ('C', 200 )],\n",
        "#             'A': [('B', 250), ('C', 400), ('S', 100 )],\n",
        "#             'B': [('A', 250), ('C', 250), ('S', 400 )],\n",
        "#             'C': [('A', 400), ('B', 250), ('S', 200 )]\n",
        "#         }\n",
        "        self.map = {\n",
        "            'S': [('A', 300), ('B', 100), ('C', 200 )],\n",
        "            'A': [('S', 300), ('B', 100), ('E', 100 ), ('D', 100 )],\n",
        "            'B': [('S', 100), ('A', 100), ('C', 50 ), ('K', 200 )],\n",
        "            'C': [('S', 200), ('B', 50), ('M', 100 ), ('L', 200 )],\n",
        "            'D': [('A', 100), ('F', 50)],\n",
        "            'E': [('A', 100), ('F', 100), ('H', 100)],\n",
        "            'F': [('D', 50), ('E', 100), ('G', 200)],\n",
        "            'G': [('F', 200), ('O', 300)],\n",
        "            'H': [('E', 100), ('K', 300)],\n",
        "            'K': [('B', 200), ('H', 300)],\n",
        "            'L': [('C', 200), ('M', 50)],\n",
        "            'M': [('C', 100), ('L', 50), ('N', 100)],\n",
        "            'N': [('M', 100), ('O', 100)],\n",
        "            'O': [('N', 100), ('G', 300)]\n",
        "        }\n",
        "        max_paths = 4\n",
        "        self.action_space = spaces.Discrete(max_paths)\n",
        "      \n",
        "        positions = len(self.map)\n",
        "        # observations: position, reward of all 4 local paths, rest reward of all locations\n",
        "        # non existing path is -1000 and no position change\n",
        "        # look at what #getObservation returns if you are confused\n",
        "        low = np.append(np.append([0], np.full(max_paths, -1000)), np.full(positions, 0))\n",
        "        high = np.append(np.append([positions - 1], np.full(max_paths, 1000)), np.full(positions, 1000))\n",
        "        self.observation_space = spaces.Box(low=low,\n",
        "                                             high=high,\n",
        "                                             dtype=np.float32)\n",
        "        self.reward_range = (-1, 1)\n",
        "\n",
        "        self.totalReward = 0\n",
        "        self.stepCount = 0\n",
        "        self.isDone = False\n",
        "\n",
        "        self.envReward = 0\n",
        "        self.envEpisodeCount = 0\n",
        "        self.envStepCount = 0\n",
        "\n",
        "        self.reset()\n",
        "        self.optimum = self.calculate_customers_reward()\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def iterate_path(self, state, action):\n",
        "        paths = self.map[state]\n",
        "        if action < len(paths):\n",
        "          return paths[action]\n",
        "        else:\n",
        "          # sorry, no such action, stay where you are and pay a high penalty\n",
        "          return (state, 1000)\n",
        "      \n",
        "    def step(self, action):\n",
        "        if self.debugStep:\n",
        "          pdb.set_trace()\n",
        "        destination, cost = self.iterate_path(self.state, action)\n",
        "        lastState = self.state\n",
        "        customerReward = self.customer_reward[destination]\n",
        "        reward = (customerReward - cost) / self.optimum\n",
        "\n",
        "        self.state = destination\n",
        "        self.customer_visited(destination)\n",
        "        done = (destination == 'S' and self.all_customers_visited())\n",
        "        if self.stepCount >= 200:\n",
        "          if BeraterEnv.showDone:\n",
        "            print(\"Done: stepCount >= 200\")\n",
        "          done = True\n",
        "\n",
        "        stateAsInt = state_name_to_int(self.state)\n",
        "        self.totalReward += reward\n",
        "        self.stepCount += 1\n",
        "        self.envReward += reward\n",
        "        self.envStepCount += 1\n",
        "\n",
        "        if self.showStep:\n",
        "            print( \"Episode: \" + (\"%4.0f  \" % self.envEpisodeCount) + \n",
        "                   \" Step: \" + (\"%4.0f  \" % self.stepCount) + \n",
        "                   lastState + ' --' + str(action) + '-> ' + self.state + \n",
        "                   ' R=' + (\"% 2.2f\" % reward) + ' totalR=' + (\"% 3.2f\" % self.totalReward) + \n",
        "                   ' cost=' + (\"%4.0f\" % cost) + ' customerR=' + (\"%4.0f\" % customerReward) + ' optimum=' + (\"%4.0f\" % self.optimum)      \n",
        "                   )\n",
        "\n",
        "        if done and not self.isDone:\n",
        "            self.envEpisodeCount += 1\n",
        "            if BeraterEnv.showDone:\n",
        "                episodes = BeraterEnv.envEpisodeModulo\n",
        "                if (self.envEpisodeCount % BeraterEnv.envEpisodeModulo != 0):\n",
        "                    episodes = self.envEpisodeCount % BeraterEnv.envEpisodeModulo\n",
        "                print( \"Done: \" + \n",
        "                        (\"episodes=%6.0f  \" % self.envEpisodeCount) + \n",
        "                        (\"avgSteps=%6.2f  \" % (self.envStepCount/episodes)) + \n",
        "                        (\"avgTotalReward=% 3.2f\" % (self.envReward/episodes) )\n",
        "                        )\n",
        "                if (self.envEpisodeCount%BeraterEnv.envEpisodeModulo) == 0:\n",
        "                    self.envReward = 0\n",
        "                    self.envStepCount = 0\n",
        "\n",
        "        self.isDone = done\n",
        "        observation = self.getObservation(stateAsInt)\n",
        "        info = {\"from\": self.state, \"to\": destination}\n",
        "\n",
        "        return observation, reward, done, info\n",
        "\n",
        "    def getObservation(self, position):\n",
        "        result = np.array([ position, \n",
        "                               self.getPathObservation(position, 0),\n",
        "                               self.getPathObservation(position, 1),\n",
        "                               self.getPathObservation(position, 2),\n",
        "                               self.getPathObservation(position, 3)\n",
        "                              ],\n",
        "                             dtype=np.float32)\n",
        "        all_rest_rewards = list(self.customer_reward.values())\n",
        "        result = np.append(result, all_rest_rewards)\n",
        "        return result\n",
        "\n",
        "    def getPathObservation(self, position, path):\n",
        "        source = int_to_state_name(position)\n",
        "        paths = self.map[self.state]\n",
        "        if path < len(paths):\n",
        "          target, cost = paths[path]\n",
        "          reward = self.customer_reward[target] \n",
        "          result = reward - cost\n",
        "        else:\n",
        "          result = -1000\n",
        "\n",
        "        return result\n",
        "\n",
        "    def customer_visited(self, customer):\n",
        "        self.customer_reward[customer] = 0\n",
        "\n",
        "    def all_customers_visited(self):\n",
        "        return self.calculate_customers_reward() == 0\n",
        "\n",
        "    def calculate_customers_reward(self):\n",
        "        sum = 0\n",
        "        for value in self.customer_reward.values():\n",
        "            sum += value\n",
        "        return sum\n",
        "\n",
        "      \n",
        "    def modulate_reward(self):\n",
        "      number_of_customers = len(self.map) - 1\n",
        "      number_per_consultant = int(number_of_customers/2)\n",
        "#       number_per_consultant = int(number_of_customers/1.5)\n",
        "      self.customer_reward = {\n",
        "          'S': 0\n",
        "      }\n",
        "      for customer_nr in range(1, number_of_customers + 1):\n",
        "        self.customer_reward[int_to_state_name(customer_nr)] = 0\n",
        "      \n",
        "      # every consultant only visits a few random customers\n",
        "      samples = random.sample(range(1, number_of_customers + 1), k=number_per_consultant)\n",
        "      key_list = list(self.customer_reward.keys())\n",
        "      for sample in samples:\n",
        "        self.customer_reward[key_list[sample]] = 1000\n",
        "\n",
        "      \n",
        "    def reset(self):\n",
        "        self.totalReward = 0\n",
        "        self.stepCount = 0\n",
        "        self.isDone = False\n",
        "\n",
        "        self.modulate_reward()\n",
        "        self.state = 'S'\n",
        "        return self.getObservation(state_name_to_int(self.state))\n",
        "      \n",
        "    def render(self):\n",
        "      print(self.customer_reward)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9J54w2URZIme",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BeraterEnv.showStep = berater_show_step\n",
        "BeraterEnv.showDone = berater_show_done\n",
        "BeraterEnv.debugStep = berater_debug_step"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EYaTAvAyYO-U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Register with OpenAI Gym"
      ]
    },
    {
      "metadata": {
        "id": "yhI9abUVYNrU",
        "colab_type": "code",
        "outputId": "677a0a31-e03c-42ba-c98a-42f6653023d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "if not 'isEnvRegistered' in locals():\n",
        "  env_name=\"Berater-v1\"\n",
        "  gym.envs.registration.register(id=env_name,entry_point=BeraterEnv,max_episode_steps=1000)\n",
        "  isEnvRegistered=True\n",
        "  print(\"Berater registered as '\" + env_name + \"'\")\n",
        "else:\n",
        "  print(\"Already registered\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Berater registered as 'Berater-v1'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sX8eJGcbOJ30",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# PPO TF-Agent: setup, train, visualize"
      ]
    },
    {
      "metadata": {
        "id": "bzoq0VM85p46",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Imports & Helpers"
      ]
    },
    {
      "metadata": {
        "id": "5ofYknQFRkRT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.agents.dqn import q_network\n",
        "from tf_agents.agents.ppo import ppo_agent\n",
        "from tf_agents.networks import actor_distribution_network\n",
        "from tf_agents.networks import value_network\n",
        "\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.environments import trajectory\n",
        "from tf_agents.metrics import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.utils import common\n",
        "\n",
        "from tf_agents.drivers import dynamic_episode_driver\n",
        "from tf_agents.environments import parallel_py_environment\n",
        "\n",
        "tf.compat.v1.enable_v2_behavior()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KiP6UgA65163",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@test {\"skip\": true}\n",
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "  old_show_step = BeraterEnv.showStep\n",
        "  old_show_done = BeraterEnv.showDone\n",
        "  old_debug_step = BeraterEnv.debugStep\n",
        "  BeraterEnv.showStep=False\n",
        "  BeraterEnv.showDone=False\n",
        "  BeraterEnv.debugStep=False\n",
        "  \n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  BeraterEnv.showStep=old_show_step\n",
        "  BeraterEnv.showDone=old_show_done\n",
        "  BeraterEnv.debugStep=old_debug_step\n",
        "  return avg_return.numpy()[0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NZoGMeUBr2tx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Setup and train"
      ]
    },
    {
      "metadata": {
        "id": "RNaP-8Ej4N7O",
        "colab_type": "code",
        "colab": {},
        "cellView": "both"
      },
      "cell_type": "code",
      "source": [
        "BeraterEnv.showStep=False # @param\n",
        "BeraterEnv.showDone=False # @param\n",
        "\n",
        "actor_fc_layers=(500,500,500) # @param\n",
        "value_fc_layers=(500,500,500) # @param\n",
        "\n",
        "\n",
        "# Params for collect\n",
        "collect_episodes_per_iteration=100 # @param\n",
        "num_parallel_environments=1\n",
        "replay_buffer_capacity=10001  # @param\n",
        "# Params for train\n",
        "num_train_iterations=2000 # @param\n",
        "num_epochs=5 # @param\n",
        "learning_rate=1e-4 # @param\n",
        "# Params for summaries and logging\n",
        "log_interval=1 # @param\n",
        "use_tf_functions=True\n",
        "debug_summaries=False\n",
        "summarize_grads_and_vars=False\n",
        "\n",
        "num_eval_episodes = 10  # @param\n",
        "eval_interval = 10  # @param"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uk5R1TlGZ5he",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
        "tf.compat.v1.set_random_seed(0)\n",
        "eval_py_env = suite_gym.load(env_name)\n",
        "tf_env = tf_py_environment.TFPyEnvironment( suite_gym.load(env_name))\n",
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
        "    tf_env.observation_spec(),\n",
        "    tf_env.action_spec(),\n",
        "    fc_layer_params=actor_fc_layers)\n",
        "value_net = value_network.ValueNetwork(\n",
        "    tf_env.observation_spec(), fc_layer_params=value_fc_layers)\n",
        "\n",
        "tf_agent = ppo_agent.PPOAgent(\n",
        "    tf_env.time_step_spec(),\n",
        "    tf_env.action_spec(),\n",
        "    optimizer,\n",
        "    actor_net=actor_net,\n",
        "    value_net=value_net,\n",
        "    num_epochs=num_epochs,\n",
        "    debug_summaries=debug_summaries,\n",
        "    summarize_grads_and_vars=summarize_grads_and_vars,\n",
        "    train_step_counter=global_step)\n",
        "tf_agent.initialize()\n",
        "\n",
        "eval_py_env = suite_gym.load(env_name)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
        "\n",
        "collect_policy = tf_agent.collect_policy\n",
        "\n",
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    tf_agent.collect_data_spec,\n",
        "    batch_size=num_parallel_environments,\n",
        "    max_length=replay_buffer_capacity)\n",
        "\n",
        "collect_driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
        "    tf_env,\n",
        "    collect_policy,\n",
        "    observers=[replay_buffer.add_batch],\n",
        "    num_episodes=collect_episodes_per_iteration)\n",
        "\n",
        "collect_driver.run = common.function(collect_driver.run, autograph=False)\n",
        "tf_agent.train = common.function(tf_agent.train, autograph=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B6jO7QZC39R4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1648
        },
        "outputId": "5bcba786-6f34-4e31-9fb4-9616f1162929"
      },
      "cell_type": "code",
      "source": [
        "avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\n",
        "returns=[avg_return]\n",
        "loss=[]\n",
        "\n",
        "for step in range(num_train_iterations):\n",
        "  collect_driver.run()\n",
        "  trajectories = replay_buffer.gather_all()\n",
        "  total_loss, _ = tf_agent.train(experience=trajectories)\n",
        "  replay_buffer.clear()\n",
        "\n",
        "  if step % log_interval == 0:\n",
        "    print('iteration/train_step = {}, loss = {}'.format(step, total_loss.numpy()))\n",
        "\n",
        "  if step % eval_interval == 0:\n",
        "    avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\n",
        "    print('iteration/train_step = {}, Average Return = {}'.format(step, avg_return))\n",
        "    returns.append(avg_return)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration/train_step = 0, loss = 5406.33154296875\n",
            "iteration/train_step = 0, Average Return = -6.139160633087158\n",
            "iteration/train_step = 1, loss = 5281.330078125\n",
            "iteration/train_step = 2, loss = 4672.37939453125\n",
            "iteration/train_step = 3, loss = 4804.7236328125\n",
            "iteration/train_step = 4, loss = 4077.86767578125\n",
            "iteration/train_step = 5, loss = 4365.21142578125\n",
            "iteration/train_step = 6, loss = 3338.60888671875\n",
            "iteration/train_step = 7, loss = 2409.446533203125\n",
            "iteration/train_step = 8, loss = 2092.705810546875\n",
            "iteration/train_step = 9, loss = 1921.863037109375\n",
            "iteration/train_step = 10, loss = 1472.182373046875\n",
            "iteration/train_step = 10, Average Return = -3.1566643714904785\n",
            "iteration/train_step = 11, loss = 1138.1549072265625\n",
            "iteration/train_step = 12, loss = 986.9594116210938\n",
            "iteration/train_step = 13, loss = 690.580810546875\n",
            "iteration/train_step = 14, loss = 346.8659973144531\n",
            "iteration/train_step = 15, loss = 287.37872314453125\n",
            "iteration/train_step = 16, loss = 186.26429748535156\n",
            "iteration/train_step = 17, loss = 195.89564514160156\n",
            "iteration/train_step = 18, loss = 88.22132873535156\n",
            "iteration/train_step = 19, loss = 110.98611450195312\n",
            "iteration/train_step = 20, loss = 103.74037170410156\n",
            "iteration/train_step = 20, Average Return = -2.3491673469543457\n",
            "iteration/train_step = 21, loss = 92.38302612304688\n",
            "iteration/train_step = 22, loss = 93.17154693603516\n",
            "iteration/train_step = 23, loss = 98.66614532470703\n",
            "iteration/train_step = 24, loss = 95.74091339111328\n",
            "iteration/train_step = 25, loss = 124.76148223876953\n",
            "iteration/train_step = 26, loss = 112.1095199584961\n",
            "iteration/train_step = 27, loss = 71.90638732910156\n",
            "iteration/train_step = 28, loss = 63.943572998046875\n",
            "iteration/train_step = 29, loss = 55.51409912109375\n",
            "iteration/train_step = 30, loss = 59.7044563293457\n",
            "iteration/train_step = 30, Average Return = -3.256671905517578\n",
            "iteration/train_step = 31, loss = 47.955841064453125\n",
            "iteration/train_step = 32, loss = 50.98125457763672\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-cbf35569e27c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_train_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mcollect_driver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mtrajectories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrajectories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    409\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1268\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1270\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m     \"\"\"\n\u001b[1;32m    555\u001b[0m     return self._call_flat(\n\u001b[0;32m--> 556\u001b[0;31m         (t for t in nest.flatten((args, kwargs))\n\u001b[0m\u001b[1;32m    557\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m    558\u001b[0m                            resource_variable_ops.ResourceVariable))))\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    414\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    415\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 416\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    417\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     59\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "vMDJZNG3akLF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualize"
      ]
    },
    {
      "metadata": {
        "id": "mTI4jEGaVTbW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "2a87550b-9df5-4bdd-ddf6-b14d578fc040"
      },
      "cell_type": "code",
      "source": [
        "#@test {\"skip\": true}\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "steps = range(0, len(returns)*eval_interval, eval_interval)\n",
        "plt.plot(steps, returns)\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Step')\n",
        "plt.ylim(top=1,bottom=-5)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-5, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAFcCAYAAADlIuYrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}