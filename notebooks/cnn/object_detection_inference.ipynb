{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "inference_tf2_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DJCordhose/ml-workshop/blob/master/notebooks/cnn/object_detection_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOvvWAVTkMR7"
      },
      "source": [
        "# Intro to Object Detection Colab\n",
        "\n",
        "Welcome to the object detection colab!  This demo will take you through the steps of running an \"out-of-the-box\" detection model on a collection of images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPs64QA1Zdov"
      },
      "source": [
        "## Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w23nHJd9reFy",
        "outputId": "fcdf0efa-f77c-474c-98c0-83bc2055a38c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.7.0'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrGfIezDrei2",
        "outputId": "20fdf44a-f266-4769-dec0-a30a77d21814",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oi28cqGGFWnY",
        "outputId": "4356b47c-1903-4b91-bf40-f22307d915c4"
      },
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "# Clone the tensorflow models repository if it doesn't already exist\n",
        "if \"models\" in pathlib.Path.cwd().parts:\n",
        "  while \"models\" in pathlib.Path.cwd().parts:\n",
        "    os.chdir('..')\n",
        "elif not pathlib.Path('models').exists():\n",
        "  !git clone --depth 1 https://github.com/tensorflow/models"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'models'...\n",
            "remote: Enumerating objects: 3135, done.\u001b[K\n",
            "remote: Counting objects: 100% (3135/3135), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2648/2648), done.\u001b[K\n",
            "remote: Total 3135 (delta 809), reused 1343 (delta 443), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (3135/3135), 33.35 MiB | 22.25 MiB/s, done.\n",
            "Resolving deltas: 100% (809/809), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Vy7UYCKrU1-",
        "outputId": "bb0c9124-358f-4eac-e9b9-c8159ef785ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install --upgrade pip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting pip\n",
            "  Downloading pip-21.3.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 6.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 21.1.3\n",
            "    Uninstalling pip-21.1.3:\n",
            "      Successfully uninstalled pip-21.1.3\n",
            "Successfully installed pip-21.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwdsBdGhFanc",
        "outputId": "714bbf37-8d99-4707-f8dd-145005739c39"
      },
      "source": [
        "# Install the Object Detection API\n",
        "%%bash\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "cp object_detection/packages/tf2/setup.py .\n",
        "python -m pip install --use-feature=2020-resolver ."
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing /content/models/research\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting avro-python3\n",
            "  Downloading avro-python3-1.10.2.tar.gz (38 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting apache-beam\n",
            "  Downloading apache_beam-2.34.0-cp37-cp37m-manylinux2010_x86_64.whl (9.8 MB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (7.1.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (4.2.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (3.2.2)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (0.29.24)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (0.5.5)\n",
            "Collecting tf-slim\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.15.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (2.0.2)\n",
            "Collecting lvis\n",
            "  Downloading lvis-0.5.3-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.1.5)\n",
            "Collecting tf-models-official>=2.5.1\n",
            "  Downloading tf_models_official-2.7.0-py2.py3-none-any.whl (1.8 MB)\n",
            "Collecting tensorflow_io\n",
            "  Downloading tensorflow_io-0.22.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (22.7 MB)\n",
            "Collecting keras==2.6.0\n",
            "  Downloading keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n",
            "Collecting opencv-python-headless\n",
            "  Downloading opencv_python_headless-4.5.4.60-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.6 MB)\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.1.3)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "Requirement already satisfied: tensorflow>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (2.7.0)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.5.0)\n",
            "Collecting tensorflow-text>=2.7.0\n",
            "  Downloading tensorflow_text-2.7.3-cp37-cp37m-manylinux2010_x86_64.whl (4.9 MB)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (5.4.8)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.12.0)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.5.12)\n",
            "Collecting py-cpuinfo>=3.3.0\n",
            "  Downloading py-cpuinfo-8.0.0.tar.gz (99 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting tensorflow-model-optimization>=0.4.1\n",
            "  Downloading tensorflow_model_optimization-0.7.0-py2.py3-none-any.whl (213 kB)\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.15.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.0.1)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.19.5)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.12.8)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->object-detection==0.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->object-detection==0.1) (2018.9)\n",
            "Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from tf-slim->object-detection==0.1) (0.12.0)\n",
            "Collecting hdfs<3.0.0,>=2.1.0\n",
            "  Downloading hdfs-2.6.0-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: protobuf<4,>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (3.17.3)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (1.3.0)\n",
            "Collecting requests<3.0.0,>=2.24.0\n",
            "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
            "Collecting dill<0.3.2,>=0.3.1.1\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting orjson<4.0\n",
            "  Downloading orjson-3.6.4-cp37-cp37m-manylinux_2_24_x86_64.whl (249 kB)\n",
            "Requirement already satisfied: pyarrow<6.0.0,>=0.15.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (3.0.0)\n",
            "Collecting fastavro<2,>=0.21.4\n",
            "  Downloading fastavro-1.4.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "Requirement already satisfied: grpcio<2,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (1.42.0)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (1.7)\n",
            "Collecting avro-python3\n",
            "  Downloading avro-python3-1.9.2.1.tar.gz (37 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting future<1.0.0,>=0.18.2\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: typing-extensions<4,>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (3.10.0.2)\n",
            "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (3.12.1)\n",
            "Requirement already satisfied: httplib2<0.20.0,>=0.8 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (0.17.4)\n",
            "Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (1.3.2)\n",
            "Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (4.1.2.30)\n",
            "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (3.0.6)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools->object-detection==0.1) (57.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.22.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_io->object-detection==0.1) (0.22.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.0.1)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.26.3)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.0.4)\n",
            "Requirement already satisfied: google-auth>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.35.0)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam->object-detection==0.1) (0.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (4.62.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2021.10.8)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (5.0.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.24.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client->tf-models-official>=2.5.1->object-detection==0.1) (0.2.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client->tf-models-official>=2.5.1->object-detection==0.1) (0.4.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client->tf-models-official>=2.5.1->object-detection==0.1) (4.7.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (2.0.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (2.10)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.7.0->tf-models-official>=2.5.1->object-detection==0.1) (2.7.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.7.0->tf-models-official>=2.5.1->object-detection==0.1) (0.37.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.7.0->tf-models-official>=2.5.1->object-detection==0.1) (1.1.2)\n",
            "INFO: pip is looking at multiple versions of requests to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting requests<3.0.0,>=2.24.0\n",
            "  Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
            "INFO: pip is looking at multiple versions of pyyaml to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "INFO: pip is looking at multiple versions of pytz to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting pytz>=2017.2\n",
            "  Downloading pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
            "INFO: pip is looking at multiple versions of python-dateutil to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting python-dateutil>=2.7.3\n",
            "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "INFO: pip is looking at multiple versions of pyparsing to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting pyparsing>=2.4.0\n",
            "  Downloading pyparsing-3.0.6-py3-none-any.whl (97 kB)\n",
            "INFO: pip is looking at multiple versions of pymongo to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting pymongo<4.0.0,>=3.8.0\n",
            "  Downloading pymongo-3.12.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
            "INFO: pip is looking at multiple versions of pydot to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting pydot<2,>=1.2.0\n",
            "  Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
            "INFO: pip is looking at multiple versions of pyarrow to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting pyarrow<6.0.0,>=0.15.1\n",
            "  Downloading pyarrow-5.0.0-cp37-cp37m-manylinux2014_x86_64.whl (23.6 MB)\n",
            "INFO: pip is looking at multiple versions of py-cpuinfo to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting py-cpuinfo>=3.3.0\n",
            "  Downloading py-cpuinfo-7.0.0.tar.gz (95 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "INFO: pip is looking at multiple versions of psutil to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting psutil>=5.4.3\n",
            "  Downloading psutil-5.8.0-cp37-cp37m-manylinux2010_x86_64.whl (296 kB)\n",
            "INFO: pip is looking at multiple versions of protobuf to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting protobuf<4,>=3.12.2\n",
            "  Downloading protobuf-3.19.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "INFO: pip is looking at multiple versions of orjson to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting orjson<4.0\n",
            "  Downloading orjson-3.6.3-cp37-cp37m-manylinux_2_24_x86_64.whl (234 kB)\n",
            "INFO: pip is looking at multiple versions of opencv-python to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opencv-python>=4.1.0.25\n",
            "  Downloading opencv_python-4.5.4.60-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.3 MB)\n",
            "INFO: pip is looking at multiple versions of oauth2client to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting oauth2client\n",
            "  Downloading oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
            "INFO: pip is looking at multiple versions of numpy to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting numpy>=1.15.4\n",
            "  Downloading numpy-1.20.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.3 MB)\n",
            "INFO: pip is looking at multiple versions of kiwisolver to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting kiwisolver>=1.1.0\n",
            "  Downloading kiwisolver-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\n",
            "INFO: pip is looking at multiple versions of kaggle to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting kaggle>=1.3.9\n",
            "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "INFO: pip is looking at multiple versions of httplib2 to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting httplib2<0.20.0,>=0.8\n",
            "  Downloading httplib2-0.19.1-py3-none-any.whl (95 kB)\n",
            "INFO: pip is looking at multiple versions of hdfs to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting hdfs<3.0.0,>=2.1.0\n",
            "  Downloading hdfs-2.5.8.tar.gz (41 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "INFO: pip is looking at multiple versions of grpcio to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting grpcio<2,>=1.29.0\n",
            "  Downloading grpcio-1.42.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "INFO: pip is looking at multiple versions of google-api-python-client to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting google-api-python-client>=1.6.7\n",
            "  Downloading google_api_python_client-2.31.0-py2.py3-none-any.whl (7.8 MB)\n",
            "INFO: pip is looking at multiple versions of future to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of fastavro to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting fastavro<2,>=0.21.4\n",
            "  Downloading fastavro-1.4.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "INFO: pip is looking at multiple versions of dill to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of cycler to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting cycler>=0.10.0\n",
            "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
            "INFO: pip is looking at multiple versions of crcmod to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting crcmod<2.0,>=1.7\n",
            "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "INFO: pip is looking at multiple versions of absl-py to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting absl-py>=0.2.2\n",
            "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
            "INFO: pip is looking at multiple versions of tensorflow-io-gcs-filesystem to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tensorflow-io-gcs-filesystem==0.22.0\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.22.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
            "INFO: pip is looking at multiple versions of tensorflow-io to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tensorflow_io\n",
            "  Downloading tensorflow_io-0.21.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (22.7 MB)\n",
            "INFO: pip is looking at multiple versions of pycocotools to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting pycocotools\n",
            "  Downloading pycocotools-2.0.3.tar.gz (106 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "INFO: pip is looking at multiple versions of pillow to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting pillow\n",
            "  Downloading Pillow-8.4.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "INFO: pip is looking at multiple versions of lxml to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting lxml\n",
            "  Downloading lxml-4.6.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.3 MB)\n",
            "INFO: pip is looking at multiple versions of six to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting six\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "INFO: pip is looking at multiple versions of matplotlib to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.5.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
            "INFO: pip is looking at multiple versions of lvis to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting lvis\n",
            "  Downloading lvis-0.5.2-py3-none-any.whl (14 kB)\n",
            "INFO: pip is looking at multiple versions of cython to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting Cython\n",
            "  Downloading Cython-0.29.24-cp37-cp37m-manylinux1_x86_64.whl (2.0 MB)\n",
            "INFO: pip is looking at multiple versions of contextlib2 to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting contextlib2\n",
            "  Downloading contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\n",
            "INFO: pip is looking at multiple versions of avro-python3 to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting avro-python3\n",
            "  Downloading avro-python3-1.9.1.tar.gz (36 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "INFO: pip is looking at multiple versions of apache-beam to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting apache-beam\n",
            "  Downloading apache_beam-2.33.0-cp37-cp37m-manylinux2010_x86_64.whl (9.8 MB)\n",
            "INFO: pip is looking at multiple versions of tf-slim to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of scipy to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting scipy\n",
            "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
            "INFO: pip is looking at multiple versions of pandas to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting pandas\n",
            "  Downloading pandas-1.3.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "INFO: pip is looking at multiple versions of tf-models-official to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tf-models-official>=2.5.1\n",
            "  Downloading tf_models_official-2.6.0-py2.py3-none-any.whl (1.8 MB)\n",
            "Collecting tensorflow>=2.5.0\n",
            "  Downloading tensorflow-2.6.2-cp37-cp37m-manylinux2010_x86_64.whl (458.3 MB)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.3.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.6.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.1.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.2.0)\n",
            "Collecting tensorboard<2.7,>=2.6.0\n",
            "  Downloading tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n",
            "Collecting typing-extensions~=3.7.4\n",
            "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.4.0)\n",
            "Collecting flatbuffers~=1.12.0\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Collecting wrapt~=1.12.1\n",
            "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting clang~=5.0\n",
            "  Downloading clang-5.0.tar.gz (30 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.1.0)\n",
            "Collecting tensorflow-estimator<2.7,>=2.6.0\n",
            "  Downloading tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\n",
            "Collecting tensorflow-io-gcs-filesystem==0.21.0\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.21.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official>=2.5.1->object-detection==0.1) (0.1.6)\n",
            "Collecting tensorflow-text>=2.5.0\n",
            "  Downloading tensorflow_text-2.7.0-cp37-cp37m-manylinux2010_x86_64.whl (4.9 MB)\n",
            "  Downloading tensorflow_text-2.6.0-cp37-cp37m-manylinux1_x86_64.whl (4.4 MB)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu->tf-models-official>=2.5.1->object-detection==0.1) (2019.12.20)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->tf-models-official>=2.5.1->object-detection==0.1) (0.8.9)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.0.1)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->tf-models-official>=2.5.1->object-detection==0.1) (2.7.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (2.3)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (21.2.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (5.4.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (1.4.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (21.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.53.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (4.2.4)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.1.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.3.6)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (3.6.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (4.8.2)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.1.1)\n",
            "Building wheels for collected packages: object-detection, avro-python3, dill, future, py-cpuinfo, seqeval, clang, wrapt\n",
            "  Building wheel for object-detection (setup.py): started\n",
            "  Building wheel for object-detection (setup.py): finished with status 'done'\n",
            "  Created wheel for object-detection: filename=object_detection-0.1-py3-none-any.whl size=21901864 sha256=fb776a7ceb4c2ebabae38ac7761939df43e3e9da887f6c8cd7aea8674669d8fa\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_z44rc8l/wheels/fa/a4/d2/e9a5057e414fd46c8e543d2706cd836d64e1fcd9eccceb2329\n",
            "  Building wheel for avro-python3 (setup.py): started\n",
            "  Building wheel for avro-python3 (setup.py): finished with status 'done'\n",
            "  Created wheel for avro-python3: filename=avro_python3-1.9.2.1-py3-none-any.whl size=43512 sha256=c71cabd82ea51a473353c9783fe240d4de31d351e4f28b30ced6e66e78c6ab5a\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/49/5f/fdb5b9d85055c478213e0158ac122b596816149a02d82e0ab1\n",
            "  Building wheel for dill (setup.py): started\n",
            "  Building wheel for dill (setup.py): finished with status 'done'\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78546 sha256=205cd304a8282fc0294955344a122536d10f33abb01bb3f1261725cdc467bd5c\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/61/fd/c57e374e580aa78a45ed78d5859b3a44436af17e22ca53284f\n",
            "  Building wheel for future (setup.py): started\n",
            "  Building wheel for future (setup.py): finished with status 'done'\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=d4697ead1a4aae096e45daf475a1161044d9302f01ad1331595a8d6b43cd90fa\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "  Building wheel for py-cpuinfo (setup.py): started\n",
            "  Building wheel for py-cpuinfo (setup.py): finished with status 'done'\n",
            "  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-py3-none-any.whl size=22258 sha256=e9c7830ed4ca67948bcee7ac1b15917c8a0770f089561dfc7f3ab14edc910a01\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/f1/1f/041add21dc9c4220157f1bd2bd6afe1f1a49524c3396b94401\n",
            "  Building wheel for seqeval (setup.py): started\n",
            "  Building wheel for seqeval (setup.py): finished with status 'done'\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=bc1389a2b92938459644798d572660d82e82958024d8f282fad87489ba879371\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "  Building wheel for clang (setup.py): started\n",
            "  Building wheel for clang (setup.py): finished with status 'done'\n",
            "  Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30692 sha256=7b93886f2d25dc652ca5fd31051db79e5e2bf6ee7ccf4e599da60467ece40a68\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/91/04/971b4c587cf47ae952b108949b46926f426c02832d120a082a\n",
            "  Building wheel for wrapt (setup.py): started\n",
            "  Building wheel for wrapt (setup.py): finished with status 'done'\n",
            "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=68716 sha256=08df32d5bf0c9695e1255d71e6509a6181c6167434c96f7f08a6942107560989\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
            "Successfully built object-detection avro-python3 dill future py-cpuinfo seqeval clang wrapt\n",
            "Installing collected packages: typing-extensions, requests, wrapt, tensorflow-estimator, tensorboard, keras, flatbuffers, clang, tensorflow, portalocker, future, dill, colorama, tf-slim, tensorflow-text, tensorflow-model-optimization, tensorflow-io-gcs-filesystem, tensorflow-addons, seqeval, sentencepiece, sacrebleu, pyyaml, py-cpuinfo, orjson, opencv-python-headless, hdfs, fastavro, avro-python3, tf-models-official, tensorflow-io, lvis, apache-beam, object-detection\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 3.10.0.2\n",
            "    Uninstalling typing-extensions-3.10.0.2:\n",
            "      Successfully uninstalled typing-extensions-3.10.0.2\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.13.3\n",
            "    Uninstalling wrapt-1.13.3:\n",
            "      Successfully uninstalled wrapt-1.13.3\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.7.0\n",
            "    Uninstalling tensorboard-2.7.0:\n",
            "      Successfully uninstalled tensorboard-2.7.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.7.0\n",
            "    Uninstalling keras-2.7.0:\n",
            "      Successfully uninstalled keras-2.7.0\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 2.0\n",
            "    Uninstalling flatbuffers-2.0:\n",
            "      Successfully uninstalled flatbuffers-2.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.7.0\n",
            "    Uninstalling tensorflow-2.7.0:\n",
            "      Successfully uninstalled tensorflow-2.7.0\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.4\n",
            "    Uninstalling dill-0.3.4:\n",
            "      Successfully uninstalled dill-0.3.4\n",
            "  Attempting uninstall: tensorflow-io-gcs-filesystem\n",
            "    Found existing installation: tensorflow-io-gcs-filesystem 0.22.0\n",
            "    Uninstalling tensorflow-io-gcs-filesystem-0.22.0:\n",
            "      Successfully uninstalled tensorflow-io-gcs-filesystem-0.22.0\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed apache-beam-2.34.0 avro-python3-1.9.2.1 clang-5.0 colorama-0.4.4 dill-0.3.1.1 fastavro-1.4.7 flatbuffers-1.12 future-0.18.2 hdfs-2.6.0 keras-2.6.0 lvis-0.5.3 object-detection-0.1 opencv-python-headless-4.5.4.60 orjson-3.6.4 portalocker-2.3.2 py-cpuinfo-8.0.0 pyyaml-6.0 requests-2.26.0 sacrebleu-2.0.0 sentencepiece-0.1.96 seqeval-1.2.2 tensorboard-2.6.0 tensorflow-2.6.2 tensorflow-addons-0.15.0 tensorflow-estimator-2.6.0 tensorflow-io-0.21.0 tensorflow-io-gcs-filesystem-0.21.0 tensorflow-model-optimization-0.7.0 tensorflow-text-2.6.0 tf-models-official-2.6.0 tf-slim-1.1.0 typing-extensions-3.7.4.3 wrapt-1.12.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: --use-feature=2020-resolver no longer has any effect, since it is now the default dependency resolver in pip. This will become an error in pip 21.0.\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "multiprocess 0.70.12.2 requires dill>=0.3.4, but you have dill 0.3.1.1 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn5_uV1HLvaz"
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import io\n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "from six import BytesIO\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import config_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "from object_detection.builders import model_builder\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IogyryF2lFBL"
      },
      "source": [
        "## Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-y9R0Xllefec"
      },
      "source": [
        "def load_image_into_numpy_array(path):\n",
        "  \"\"\"Load an image from file into a numpy array.\n",
        "\n",
        "  Puts image into numpy array to feed into tensorflow graph.\n",
        "  Note that by convention we put it into a numpy array with shape\n",
        "  (height, width, channels), where channels=3 for RGB.\n",
        "\n",
        "  Args:\n",
        "    path: the file path to the image\n",
        "\n",
        "  Returns:\n",
        "    uint8 numpy array with shape (img_height, img_width, 3)\n",
        "  \"\"\"\n",
        "  img_data = tf.io.gfile.GFile(path, 'rb').read()\n",
        "  image = Image.open(BytesIO(img_data))\n",
        "  (im_width, im_height) = image.size\n",
        "  return np.array(image.getdata()).reshape(\n",
        "      (im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "def get_keypoint_tuples(eval_config):\n",
        "  \"\"\"Return a tuple list of keypoint edges from the eval config.\n",
        "  \n",
        "  Args:\n",
        "    eval_config: an eval config containing the keypoint edges\n",
        "  \n",
        "  Returns:\n",
        "    a list of edge tuples, each in the format (start, end)\n",
        "  \"\"\"\n",
        "  tuple_list = []\n",
        "  kp_list = eval_config.keypoint_edge\n",
        "  for edge in kp_list:\n",
        "    tuple_list.append((edge.start, edge.end))\n",
        "  return tuple_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4YjnOjME1gy"
      },
      "source": [
        "# @title Choose the model to use, then evaluate the cell.\n",
        "MODELS = {'centernet_with_keypoints': 'centernet_hg104_512x512_kpts_coco17_tpu-32', 'centernet_without_keypoints': 'centernet_hg104_512x512_coco17_tpu-8'}\n",
        "\n",
        "model_display_name = 'centernet_with_keypoints' # @param ['centernet_with_keypoints', 'centernet_without_keypoints']\n",
        "model_name = MODELS[model_display_name]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6917xnUSlp9x"
      },
      "source": [
        "### Build a detection model and load pre-trained model weights\n",
        "\n",
        "This sometimes takes a little while, please be patient!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctPavqlyPuU_"
      },
      "source": [
        "# Download the checkpoint and put it into models/research/object_detection/test_data/\n",
        "\n",
        "if model_display_name == 'centernet_with_keypoints':\n",
        "  !wget http://download.tensorflow.org/models/object_detection/tf2/20200711/centernet_hg104_512x512_kpts_coco17_tpu-32.tar.gz\n",
        "  !tar -xf centernet_hg104_512x512_kpts_coco17_tpu-32.tar.gz\n",
        "  !mv centernet_hg104_512x512_kpts_coco17_tpu-32/checkpoint models/research/object_detection/test_data/\n",
        "else:\n",
        "  !wget http://download.tensorflow.org/models/object_detection/tf2/20200711/centernet_hg104_512x512_coco17_tpu-8.tar.gz\n",
        "  !tar -xf centernet_hg104_512x512_coco17_tpu-8.tar.gz\n",
        "  !mv centernet_hg104_512x512_coco17_tpu-8/checkpoint models/research/object_detection/test_data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cni4SSocvP_"
      },
      "source": [
        "pipeline_config = os.path.join('models/research/object_detection/configs/tf2/',\n",
        "                                model_name + '.config')\n",
        "model_dir = 'models/research/object_detection/test_data/checkpoint/'\n",
        "\n",
        "# Load pipeline config and build a detection model\n",
        "configs = config_util.get_configs_from_pipeline_file(pipeline_config)\n",
        "model_config = configs['model']\n",
        "detection_model = model_builder.build(\n",
        "      model_config=model_config, is_training=False)\n",
        "\n",
        "# Restore checkpoint\n",
        "ckpt = tf.compat.v2.train.Checkpoint(\n",
        "      model=detection_model)\n",
        "ckpt.restore(os.path.join(model_dir, 'ckpt-0')).expect_partial()\n",
        "\n",
        "def get_model_detection_function(model):\n",
        "  \"\"\"Get a tf.function for detection.\"\"\"\n",
        "\n",
        "  @tf.function\n",
        "  def detect_fn(image):\n",
        "    \"\"\"Detect objects in image.\"\"\"\n",
        "\n",
        "    image, shapes = model.preprocess(image)\n",
        "    prediction_dict = model.predict(image, shapes)\n",
        "    detections = model.postprocess(prediction_dict, shapes)\n",
        "\n",
        "    return detections, prediction_dict, tf.reshape(shapes, [-1])\n",
        "\n",
        "  return detect_fn\n",
        "\n",
        "detect_fn = get_model_detection_function(detection_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKtD0IeclbL5"
      },
      "source": [
        "# Load label map data (for plotting).\n",
        "\n",
        "Label maps correspond index numbers to category names, so that when our convolution network predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mucYUS6exUJ"
      },
      "source": [
        "label_map_path = configs['eval_input_config'].label_map_path\n",
        "label_map = label_map_util.load_labelmap(label_map_path)\n",
        "categories = label_map_util.convert_label_map_to_categories(\n",
        "    label_map,\n",
        "    max_num_classes=label_map_util.get_max_label_map_index(label_map),\n",
        "    use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)\n",
        "label_map_dict = label_map_util.get_label_map_dict(label_map, use_display_name=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLusV1o-mAx8"
      },
      "source": [
        "### Putting everything together!\n",
        "\n",
        "Run the below code which loads an image, runs it through the detection model and visualizes the detection results, including the keypoints.\n",
        "\n",
        "Note that this will take a long time (several minutes) the first time you run this code due to tf.function's trace-compilation --- on subsequent runs (e.g. on new images), things will be faster.\n",
        "\n",
        "Here are some simple things to try out if you are curious:\n",
        "* Try running inference on your own images (local paths work)\n",
        "* Modify some of the input images and see if detection still works.  Some simple things to try out here (just uncomment the relevant portions of code) include flipping the image horizontally, or converting to grayscale (note that we still expect the input image to have 3 channels).\n",
        "* Print out `detections['detection_boxes']` and try to match the box locations to the boxes in the image.  Notice that coordinates are given in normalized form (i.e., in the interval [0, 1]).\n",
        "* Set min_score_thresh to other values (between 0 and 1) to allow more detections in or to filter out more detections.\n",
        "\n",
        "Note that you can run this cell repeatedly without rerunning earlier cells.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vr_Fux-gfaG9"
      },
      "source": [
        "image_dir = 'models/research/object_detection/test_images/'\n",
        "image_path = os.path.join(image_dir, 'image2.jpg')\n",
        "image_np = load_image_into_numpy_array(image_path)\n",
        "\n",
        "# Things to try:\n",
        "# Flip horizontally\n",
        "# image_np = np.fliplr(image_np).copy()\n",
        "\n",
        "# Convert image to grayscale\n",
        "# image_np = np.tile(\n",
        "#     np.mean(image_np, 2, keepdims=True), (1, 1, 3)).astype(np.uint8)\n",
        "\n",
        "input_tensor = tf.convert_to_tensor(\n",
        "    np.expand_dims(image_np, 0), dtype=tf.float32)\n",
        "detections, predictions_dict, shapes = detect_fn(input_tensor)\n",
        "\n",
        "label_id_offset = 1\n",
        "image_np_with_detections = image_np.copy()\n",
        "\n",
        "# Use keypoints if available in detections\n",
        "keypoints, keypoint_scores = None, None\n",
        "if 'detection_keypoints' in detections:\n",
        "  keypoints = detections['detection_keypoints'][0].numpy()\n",
        "  keypoint_scores = detections['detection_keypoint_scores'][0].numpy()\n",
        "\n",
        "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np_with_detections,\n",
        "      detections['detection_boxes'][0].numpy(),\n",
        "      (detections['detection_classes'][0].numpy() + label_id_offset).astype(int),\n",
        "      detections['detection_scores'][0].numpy(),\n",
        "      category_index,\n",
        "      use_normalized_coordinates=True,\n",
        "      max_boxes_to_draw=200,\n",
        "      min_score_thresh=.30,\n",
        "      agnostic_mode=False,\n",
        "      keypoints=keypoints,\n",
        "      keypoint_scores=keypoint_scores,\n",
        "      keypoint_edges=get_keypoint_tuples(configs['eval_config']))\n",
        "\n",
        "plt.figure(figsize=(12,16))\n",
        "plt.imshow(image_np_with_detections)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYnOxprty3TD"
      },
      "source": [
        "## Digging into the model's intermediate predictions\n",
        "\n",
        "For this part we will assume that the detection model is a CenterNet model following Zhou et al (https://arxiv.org/abs/1904.07850).  And more specifically, we will assume that `detection_model` is of type `meta_architectures.center_net_meta_arch.CenterNetMetaArch`.\n",
        "\n",
        "As one of its intermediate predictions, CenterNet produces a heatmap of box centers for each class (for example, it will produce a heatmap whose size is proportional to that of the image that lights up at the center of each, e.g., \"zebra\"). In the following, we will visualize these intermediate class center heatmap predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBgYgSGMhHVi"
      },
      "source": [
        "if detection_model.__class__.__name__ != 'CenterNetMetaArch':\n",
        "  raise AssertionError('The meta-architecture for this section '\n",
        "  'is assumed to be CenterNetMetaArch!')\n",
        "\n",
        "def get_heatmap(predictions_dict, class_name):\n",
        "  \"\"\"Grabs class center logits and apply inverse logit transform.\n",
        "\n",
        "  Args:\n",
        "    predictions_dict: dictionary of tensors containing a `object_center`\n",
        "      field of shape [1, heatmap_width, heatmap_height, num_classes]\n",
        "    class_name: string name of category (e.g., `horse`)\n",
        "\n",
        "  Returns:\n",
        "    heatmap: 2d Tensor heatmap representing heatmap of centers for a given class\n",
        "      (For CenterNet, this is 128x128 or 256x256) with values in [0,1]\n",
        "  \"\"\"\n",
        "  class_index = label_map_dict[class_name]\n",
        "  class_center_logits = predictions_dict['object_center'][0]\n",
        "  class_center_logits = class_center_logits[0][\n",
        "    :, :, class_index - label_id_offset]\n",
        "  heatmap = tf.exp(class_center_logits) / (tf.exp(class_center_logits) + 1)\n",
        "  return heatmap\n",
        "\n",
        "def unpad_heatmap(heatmap, image_np):\n",
        "  \"\"\"Reshapes/unpads heatmap appropriately.\n",
        "\n",
        "  Reshapes/unpads heatmap appropriately to match image_np.\n",
        "\n",
        "  Args:\n",
        "    heatmap: Output of `get_heatmap`, a 2d Tensor\n",
        "    image_np: uint8 numpy array with shape (img_height, img_width, 3).  Note\n",
        "      that due to padding, the relationship between img_height and img_width\n",
        "      might not be a simple scaling.\n",
        "\n",
        "  Returns:\n",
        "    resized_heatmap_unpadded: a resized heatmap (2d Tensor) that is the same\n",
        "      size as `image_np`\n",
        "  \"\"\"\n",
        "  heatmap = tf.tile(tf.expand_dims(heatmap, 2), [1, 1, 3]) * 255\n",
        "  pre_strided_size = detection_model._stride * heatmap.shape[0]\n",
        "  resized_heatmap = tf.image.resize(\n",
        "      heatmap, [pre_strided_size, pre_strided_size],\n",
        "      method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "  resized_heatmap_unpadded = tf.slice(resized_heatmap, begin=[0,0,0], size=shapes)\n",
        "  return tf.image.resize(\n",
        "      resized_heatmap_unpadded,\n",
        "      [image_np.shape[0], image_np.shape[1]],\n",
        "      method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)[:,:,0]\n",
        "\n",
        "\n",
        "class_name = 'kite'\n",
        "heatmap = get_heatmap(predictions_dict, class_name)\n",
        "resized_heatmap_unpadded = unpad_heatmap(heatmap, image_np)\n",
        "plt.figure(figsize=(12,16))\n",
        "plt.imshow(image_np_with_detections)\n",
        "plt.imshow(resized_heatmap_unpadded, alpha=0.7,vmin=0, vmax=160, cmap='viridis')\n",
        "plt.title('Object center heatmap (class: ' + class_name + ')')\n",
        "plt.show()\n",
        "\n",
        "class_name = 'person'\n",
        "heatmap = get_heatmap(predictions_dict, class_name)\n",
        "resized_heatmap_unpadded = unpad_heatmap(heatmap, image_np)\n",
        "plt.figure(figsize=(12,16))\n",
        "plt.imshow(image_np_with_detections)\n",
        "plt.imshow(resized_heatmap_unpadded, alpha=0.7,vmin=0, vmax=160, cmap='viridis')\n",
        "plt.title('Object center heatmap (class: ' + class_name + ')')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}