<!doctype html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>CNNs</title>

    <link rel="stylesheet" href="reveal.js/css/reset.css">
    <link rel="stylesheet" href="reveal.js/css/reveal.css">
    <!-- <link rel="stylesheet" href="reveal.js/css/theme/black.css"> -->
    <link rel="stylesheet" href="reveal.js/css/theme/solarized.css">

    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="reveal.js/lib/css/monokai.css">

    <style>
        /*pre code {*/
        /*display: block;*/
        /*padding: 0.5em;*/
        /*background: #FFFFFF !important;*/
        /*color: #000000 !important;*/
        /*}*/

        .right-img {
            margin-left: 10px !important;
            float: right;
            height: 500px;
        }

        .todo:before {
            content: 'TODO: ';
        }

        .todo {
            color: red !important;
        }

        code span.line-number {
            color: lightcoral;
        }

        .reveal pre code {
            max-height: 1000px !important;
        }

        img {
            border: 0 !important;
            box-shadow: 0 0 0 0 !important;
        }

        .reveal {
            -ms-touch-action: auto !important;
            touch-action: auto !important;
        }

        .reveal h2,
        .reveal h3,
        .reveal h4 {
            letter-spacing: 2px;
            font-family: 'Amiri', serif;
            /* font-family: 'Times New Roman', Times, serif; */
            font-weight: bold;
            font-style: italic;
            letter-spacing: -2px;
            text-transform: none !important;
        }

        .reveal em {
            font-weight: bold;
        }

        .reveal .step-subtitle h1 {
            letter-spacing: 1px;
        }

        .reveal .step-subtitle h2,
        .reveal .step-subtitle h3 {
            text-transform: none;
            font-style: italic;
            font-weight: normal;
            /* font-weight: 400; */
            /* font-family: 'Amiri', serif; */
            font-family: 'Lobster', serif;
            letter-spacing: 1px;
            color: #2aa198;
            text-decoration: underline;
        }

        .reveal .front-page h1,
        .reveal .front-page h2 {
            font-family: "League Gothic";
            font-style: normal;
            text-transform: uppercase !important;
            letter-spacing: 1px;
        }

        .reveal .front-page h1 {
            font-size: 2.5em !important;
        }

        .reveal .highlight {
            background-color: #D3337B;
            color: white;
        }

        .reveal section img {
            background: none;
        }

        .reveal img.with-border {
            border: 1px solid #586e75 !important;
            box-shadow: 3px 3px 1px rgba(0, 0, 0, 0.15) !important;
        }

        .reveal li {
            margin-bottom: 8px;
        }

        /* For li's that use FontAwesome icons as bullet-point */
        .reveal ul.fa-ul li {
            list-style-type: none;
        }
    </style>


    <!-- Printing and PDF exports -->
    <script>
        var link = document.createElement('link');
        link.rel = 'stylesheet';
        link.type = 'text/css';
        var printMode = window.location.search.match(/print-pdf/gi);
        link.href = printMode ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
        document.getElementsByTagName('head')[0].appendChild(link);
    </script>
</head>

<body>
    <div class="reveal">
        <div class="slides">

<section data-markdown>
        <textarea data-template>
## Networks for Images (Convolutional Layers)
</textarea>
</section>

<section>
        <img src='img/data.png'>
</section>

<section data-markdown>
        <textarea data-template>
### Example: Fashion MNIST

Learn to recognize 28x28 grayscale images of fashion Items

<img src="img/fashion-mnist-sprite.png" height="300px">

<small>

https://colab.research.google.com/github/djcordhose/ml-workshop/blob/master/notebooks/tf2/fashion-mnist.ipynb
</small>
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Challenges for Images Recognition

1. Feeding all pixels into Dense Layers will work, but slow and many parameters 
1. Manual Feature extraction from images might work, but
   * is tedious and error prone
   * requires domain knowledge
   * needs frequent manual updates
1. Convolutional networks will learn feature extraction before passing few features to Dense Layer Classifiers

<small>

https://towardsdatascience.com/simple-introduction-to-convolutional-neural-networks-cdf8d3077bac
</small>
</textarea>
</section>

<section>
    <h3>Architectures of Convolutional Neural Networks: VGG</h3>
        <img src="img/vgg.png" height="350px">
        <p>
            <small>There are a number of specialized neural network layers</small>
        </p>
</section>

    <section>
            <h3>MNIST - Using a model <em>already trained</em></h3>
            <p>Exploring the different types layers together</p>
            <a href="https://transcranial.github.io/keras-js/#/mnist-cnn" target="_blank">
                <img src="img/keras-browser.png" height="350px">
            </a>
            <p><small>
                <a href="https://transcranial.github.io/keras-js/#/mnist-cnn" target="_blank">https://transcranial.github.io/keras-js/#/mnist-cnn</a>
            </small></p>
        </section>


<section data-markdown>
    <textarea data-template>
### Convolutional Blocks: Cascading many Convolutional Layers having down sampling in between

![Applying filters](http://cs231n.github.io/assets/cnn/cnn.jpeg)

http://cs231n.github.io/convolutional-networks/#conv
</textarea>
</section>

<section data-markdown style="font-size: x-large">
    <textarea data-template>
### Example of a Convolution
![Dog](https://github.com/DJCordhose/speed-limit-signs/raw/master/img/conv/dog.png)
#### Many convolutional filters applied over all channels
![Dog after Convolutional Filters applied](https://github.com/DJCordhose/speed-limit-signs/raw/master/img/conv/dog-conv1.png)
http://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### How Filter Kernels work

<img src='img/cnn-kernels.gif' height="480">
<small>

http://sigmoidprime.com/post/the-inner-workings-of-convolutional-nets/
<br>
https://twitter.com/wster/status/1079741301418049537
</small>
</textarea>
</section>


<section>
    <h3>How do Convolutions work - Image Kernels</h3>
    <p><small>You might know from Photoshop etc., used in Convolutional Neural Networks</small></p>
    <a href="http://setosa.io/ev/image-kernels/" target="_blank">
        <img src="img/setosa_io_image-kernels.png" height="300px">
    </a>
    <p>
        <small>
            <a href="http://setosa.io/ev/image-kernels/" target="_blank">http://setosa.io/ev/image-kernels/</a>
        </small>
    </p>
</section>


<section>
<h3>Experiment with Image Kernels</h3>
<ol>
    <li class="fragment">How can a matrix of numbers represent an image? How could you encode color?</li>
    <li class="fragment">Explain the effect the filter kernels Sharpen and Blur have on the sample image - explain the effect of the specific values to the result</li>
    <li class="fragment">Starting from the identity kernel - how can you create a filter that highlights edges on the top of shown digits? What about the bottom?</li>
</ol>
<p>
        <small>
            <a href="http://setosa.io/ev/image-kernels/" target="_blank">http://setosa.io/ev/image-kernels/</a>
            <br>
            <br>

            Sample image: <a 
            href="https://github.com/DJCordhose/speed-limit-signs/raw/master/data/real-world/4/100-sky-cutoff-detail.jpg" target="_blank">
            https://github.com/DJCordhose/speed-limit-signs/raw/master/data/real-world/4/100-sky-cutoff-detail.jpg</a>
            
        </a>
            
        </small>
    </p>
</section>

<section data-markdown>
    <textarea data-template>
### Downsampling Layer: Reduces data sizes and risk of overfitting
![Pooling](http://cs231n.github.io/assets/cnn/pool.jpeg)
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Max Pooling
![Max Pooling](http://cs231n.github.io/assets/cnn/maxpool.jpeg)
<small>
http://cs231n.github.io/convolutional-networks/#pool
</small>
</textarea>
</section>

    <section>
            <h3>Keras layers</h3>

            <p><small>Convolution</small></p>
            <pre><code contenteditable data-trim class="fragment line-numbers javascript">
    model.add(Conv2D(filters=32, activation='relu'))
                </code></pre>

                <p><small>Max Pooling</small></p>
                <pre><code contenteditable data-trim class="fragment line-numbers javascript">
model.add(MaxPooling2D())
                </code></pre>
                                    
                <p><small>Flatten 2d to make it accessible to Dense layers</small></p>
            <pre><code contenteditable data-trim class="fragment line-numbers javascript">
model.add(Flatten())
            </code></pre>
        <p>
            <small>
                    <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers">
                        https://www.tensorflow.org/api_docs/python/tf/keras/layers
                    </a>
            </small>
        </p>
    </section>

        <section data-markdown>
                <textarea data-template>
### Exercise

_Can you improve the model for Fashion MNIST notebook?_

* train for more/less epochs
* other/more/less layers
* different sequence, less/more filters
* prevent overfitting even better
* For CNN you use the same means of regularization as in other standard NNs 

<small>

https://colab.research.google.com/github/djcordhose/ml-workshop/blob/master/notebooks/tf2/fashion-mnist.ipynb
</small>
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Standard CNN Architectures

![Performance of CNN Architectures](https://cdn-images-1.medium.com/max/1600/1*kBpEOy4fzLiFxRLjpxAX6A.png)

<small>
https://medium.com/towards-data-science/neural-network-architectures-156e5bad51ba
</small>
</textarea>
</section>
    
<section>
    <h3>One Example: Google Inception V3</h3>
    <img src="img/inception_v3_architecture.png" height="400px">
    <p>
        <small>
            Paper: <a href="https://arxiv.org/abs/1409.4842" target="_blank">Going Deeper with Convolutions</a>
            <br>
            <a href="https://stackoverflow.com/questions/39352108/does-the-inception-model-have-two-softmax-outputs" target="_blank">
            Why two classifiers?</a>
        </small>
    </p>
</section>

<section data-markdown>
    <textarea data-template>
### Transfer Learning

* Keras provides a lot of pre-defined network architectures for image classification (like ResNet and MobileNet)
* You can get each of them pre-trained on a generic image data set (ImageNet)
* You can either use them as is
* Or retrain (fine tune weights) with your own images
* https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html
* Might be helpful if you do not have a lot of data

<small>
https://keras.io/applications/
</small>
    </textarea>
</section>

<section>
    <h4>ImageNet dataset to classify images</h4>
    <img src="img/imagenet.png" height="500px">
    <p><small><a href="http://image-net.org/" target="_blank">http://image-net.org/</a></small></p>
</section>

        </div>
    </div>

    <script src="reveal.js/js/reveal.js"></script>
    <script src="lib/jquery-2.2.4.js"></script>

    <script>
        $('section:not([data-background])').attr('data-background', "background/white.jpg");
    </script>
    <script>
        const isLocal = window.location.hostname.indexOf('localhost') !== -1 || 
                    window.location.hostname.indexOf('127.0.0.1') !== -1;
    
        if (isLocal && !printMode) {
        } else {
            // only applies to public version
                $('.todo').remove();
                $('.preparation').remove();
                $('.local').remove();
        }
    
        Reveal.addEventListener( 'ready', function( event ) {
            // applies to all versions
            $('code').addClass('line-numbers');
    
            $('.fragments li').addClass('fragment')
    
            // make all links open in new tab
            $('a').attr('target', '_blank')
    
            if (isLocal && !printMode) {
                // only applies to presentation version
                Reveal.configure({ controls: false });
            } else {
                // only applies to public version
                $('.fragment').removeClass('fragment');
            }
    
    
        } );
    </script>
    
    <script>
        // More info about config & dependencies:
        // - https://github.com/hakimel/reveal.js#configuration
        // - https://github.com/hakimel/reveal.js#dependencies
        Reveal.initialize({
            controls: true,
            progress: false,
            history: true,
            center: true,
            width: 1100,

            math: {
                mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
                config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
            },

            dependencies: [
                { src: 'reveal.js/plugin/markdown/marked.js' },
                { src: 'reveal.js/plugin/markdown/markdown.js' },
                { src: 'reveal.js/plugin/notes/notes.js', async: true },
                { src: 'reveal.js/plugin/highlight/highlight.js', async: true },
                { src: 'lib/js/line-numbers.js' },
                { src: 'reveal.js/plugin/math/math.js', async: true }
            ]
        });
    </script>
</body>

</html>