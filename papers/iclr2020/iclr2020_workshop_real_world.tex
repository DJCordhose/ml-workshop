
\documentclass{article} % For LaTeX2e
\usepackage{iclr2020_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}

\title{Squaring Deep Neural Networks for interpretability \\ Decision Trees as Surrogate Model for NNs}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213, USA \\
\texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
\And
Ji Q. Ren \& Yevgeny LeNet \\
Department of Computational Neuroscience \\
University of the Witwatersrand \\
Joburg, South Africa \\
\texttt{\{robot,net\}@wits.ac.za} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}

Deep Neural Networks have outstanding performance and flexibility when learning from complex data sets. 
They can be regularized to generalize well on pretty much any data set. 
However, without additional work, they are black boxes and how they come to conclusions is not transparent or comprehensible. But exactly this right to explanation is well established by 
Europe's GDPR, United States' credit score, and
many other real world applications. On the opposite side, decision trees can be much more comprehensible, and can be trained either towards high understandability (simple tree) 
or high accuracy (complex tree). Unfortunately, unlike Neural Networks they tend to overfit when trained on real world data and are hard to regularize. In this contribution I will show how training decision trees on data generated by a neural network gives us a dial to be tuned between predictive power on one side and explainability on the other side.

\end{abstract}


\section{Motivation}

Explainability ~\citep{rudin2018stop}.

\section{Problem}

Our use case is shown in figure ~\ref{fig:use_case}. From two variables we want to lean the probability class of a car accident given the age of the driver and the top speed of the car driven. 

\begin{figure}[h]
\begin{center}
\includegraphics[width=\linewidth]{data.png}
\end{center}
\caption{Simple use case: Risk Prediction.}
\label{fig:use_case}
\end{figure}




\begin{figure}[h]
\begin{center}
\includegraphics[width=\linewidth]{dt-reg-all.png}
\end{center}
\caption{Decision Boundaries by regularized decision tree.}
\end{figure}


\section{Approach}

 ~\citep{schaaf2019enhancing} propose to a special L1-Orthogonal Regularization. 

I ended up using "Self-Normalizing Neural Networks" as proposed by ~\citep{klambauer2017selfnormalizing} in combination with standard L1-Regularization on the activation level which gave the best results in terms of being reproduzed by a simple surrogate decisicion tree. Decision Boundaries of a model trained that way are shown in figure ~\ref{fig:nn-decision-boundaries}.

\begin{figure}[h]
\begin{center}
\includegraphics[width=\linewidth]{nn.png}
\end{center}
\caption{Decision Boundaries drawn by deep neural network, 72\% accuracy.}
\label{fig:nn-decision-boundaries}
\end{figure}


\section{Results}

I tried several configurations of training a two hidden layer neural networks with respect to how well they 

\begin{figure}[h]
\begin{center}
\includegraphics[width=\linewidth]{shallow-surrogate.png}
\end{center}
\caption{Decision Boundaries by shallow surrogate decision tree, still 64\% accuracy.}
\end{figure}

\begin{figure}[h]
\begin{center}
\includegraphics[width=\linewidth]{dtreeviz-prediction-path.png}
\end{center}
\caption{Explanation derived from shallow tree using dtreeviz.}
\end{figure}


\section{Conclusions}
\bibliography{iclr2020_conference}
\bibliographystyle{iclr2020_conference}

\end{document}
