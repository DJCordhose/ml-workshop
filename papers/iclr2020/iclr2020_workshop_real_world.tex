
\documentclass{article} % For LaTeX2e
\usepackage{iclr2020_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}

\title{Gaining interpretability: Deep neural networks as teachers for decision trees}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213, USA \\
\texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
\And
Ji Q. Ren \& Yevgeny LeNet \\
Department of Computational Neuroscience \\
University of the Witwatersrand \\
Joburg, South Africa \\
\texttt{\{robot,net\}@wits.ac.za} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}

Deep neural networks have outstanding performance and flexibility and can be
regularized to generalize well on pretty much any data set. However, without
additional work, they are black boxes and how they come to conclusions is not
transparent or comprehensible. But exactly this right to explanation is well established by Europe’s GDPR, United States’ credit score, and many other real world
applications. Additionally, interpretability helps debugging and evaluating the performance of a model. On the opposite side, decision trees can be much more comprehensi-
ble, and can be trained either towards high understandability (simple tree) or high
accuracy (complex tree). Unfortunately, unlike neural networks they tend to over-
fit when trained on real world data and are hard to regularize. In this contribution I
will show how training decision trees on data generated by a neural network gives
us a dial to be tuned between predictive power on one side and interpretability and
stability on the other side.
\end{abstract}

\section{Motivation and Problem Statement}

Decision trees are one type of machine learning model that allow for interpretation given the tree has low complexity. Unfortunately decision trees tend to overfit when trained on real world data. Real world data often comes from a combination of distributions that largely differ in density of samples. Some parts may be covered by a lot of samples, others just by few. So small variations in training data have a high impact on the tree. 

\begin{figure}[h]
\begin{center}
\includegraphics[width=4.0in]{dt-reg-all.png}
\end{center}
\caption{Decision Boundaries by regularized decision tree.}
\label{fig:dt_reg_bad}
\end{figure}

Figure ~\ref{fig:dt_reg_bad} shows the classification results of a decision tree for our example use case where data looks scattered in a way we just described it. From two variables we want to lean the risk class of a driver getting into a car accident given the age of the driver and the top speed of the car driven. You see the test data plottet in the foreground while the decision boundaries are plottet as the background. Darker background colors indicate higher probabilities of the prediction. Even though we apply strong regularization, there are parts of the decision tree that are simply do complicate. Thus the decision tree overfits in a way that does not allow for a good interpretation story.

So the problem at hand can be stated like this: can we keep the interpretability of a decision tree on one side and mix it with the generalization power of a deep neural network on the other side?

\section{Approach}

We start with a deep neural network as our black box model and train it to high accuracy and generalization as shown in figure ~\ref{fig:nn-decision-boundaries}. In the next step to make our model interpretable we replace it with a regularized decision tree as a global surrogate model. We use the black box model to generate a new training data set by feeding in an equidistant grid of samples over the domains of our input values and use the predictions as our new target variable. This is used as the new training data to approximate the predictions of a black box model.

It is important that we do not propose to use the deep neural network for prediction afterwards, but we only use it as an intermediate means to come up with a decision tree which can all by itself be made interpretable. We thus do not need to pay too much attention to making our deep learning model compatible with a decision tree. We are however aware of means to such regularisations as proposed by ~\citep{schaaf2019enhancing} and also some work done by the author himself. 

\begin{figure}[h]
\begin{center}
\includegraphics[width=4.0in]{nn.png}
\end{center}
\caption{Decision Boundaries drawn by deep neural network, 72\% accuracy on test and training data.}
\label{fig:nn-decision-boundaries}
\end{figure}

As it turns out the only thing that matters for our approach is that the network is properly regularized, but not so much how this is achieved. I ended up using "Self-Normalizing Neural Networks" as proposed by ~\citep{klambauer2017selfnormalizing} in combination with standard L1-regularization on the activation level. Decision Boundaries of a model trained that way are shown in figure ~\ref{fig:nn-decision-boundaries}.

\section{Results}

In figure ~\ref{fig:surrogate-model} you can see the predictions of the resulting surrogate decision tree that has been tuned for interpretability. Setting the maximum depth of the tree gives us a dial between a model as accurate as the original black box model or as interpretable as the model you are seing. So, practically the findings of our work do not back up ~\citep{rudin2018stop} claim that there is no trade-off between accuracy and interpretability. Even more this work is based on the contrary belief. To me understanding this is largely due to the unsual definition of accuracy she uses. I, however, follow her in her suggestion that explainable models that do not replace the black box models are useless at best. This work also contradicts in the judgement that instability of an interpretable model is a good thing. We will show how our approach makes models more stable even though there is room for improvement.

\begin{figure}[h]
\begin{center}
\includegraphics[width=4.0in]{shallow-surrogate.png}
\end{center}
\caption{Decision Boundaries by shallow surrogate decision tree, still 64\% accuracy.}
\label{fig:surrogate-model}
\end{figure}

The decision tree that could replicate the blackbox model by 100\% has a maximum depth of 12, while the one shown works for interpretation has three levels only and significantly less accuracy (64\% vs 72\%). In the surrogate model we observe the same amount of overfitting as in the blank box neural network (namely none) as the surrogate decision tree overfits on what the black box models presents it - which is already regularized. This also positively impacts the stability of our decision tree.

Having a model like this also allows for generation of if clauses similar to what humans would write as business logic. Figure ~\ref{fig:rules} shows one of many ways to turn a shallow decision tree into code. It would be just as easy to create code having nested if statements instead of one combined condition for each possible prediction. All the generated code blocks are equivalent in power as our decision tree and could used to completely replace it. You could even bring such a code based "model" into production.

\begin{figure}[h]
\begin{center}
\includegraphics[width=4.0in]{code.png}
\end{center}
\caption{Business logic rules automatically generated from surrogate model.}
\label{fig:rules}
\end{figure}

\section{Conclusions}

The best known way of interpreting a prediction made by a decision tree is to look at the path chosen as shown in figure ~\ref{fig:prediction-path}. The information provided for this example already is quite complex, but matches what you see in figure ~\ref{fig:surrogate-model}. Young drivers (20) with relatively fast cars (110) tend to have more accidents. This is what you can directly read off of the plot. Similar thoughs can be made for the other 6 leaf nodes of the tree, e.g. people within a certain range of age are unlikely to have a lot of accidents regardless of the max speed of their cars. Speaking variables, low complexity and shallowness of the tree are a precondition to interpretation, though.

\begin{figure}[h]
\begin{center}
\includegraphics[width=4.0in]{dtreeviz-prediction-path.png}
\end{center}
\caption{Prediction path featuring all kinds of information for interpretation.}
\label{fig:prediction-path}
\end{figure}

Practical issues arise around exactly this area of regularizing the tree to low complexity. Next to good accuracy you would also want stability of the tree. Decision trees are high variance, which means the parameters are very sensitive to small changes in the input. Since it is hard to impossible to make training of neural networks totally deterministic each training run will generate slightly differnt input data for the decision tree potentially leading to drastic changes in their split points and even overall structure. This is undeseriable as it makes interpretation much harder. Best results so far arise from manual experiments restricting both the depth and minimum leaf size which results in stable results for this use case, but there is no evidence this will be the case for other use cases as well. Special measures to stabilize trees are proposed in ~\citep{arsov2019stability} and ~\citep{last2002stability}. 

\subsubsection*{Acknowledgments}
Thanks to Terence Parr for advising on how to regularize my decision trees and providing me with the \url{https://github.com/parrt/dtreeviz} tool used to plot figure ~\ref{fig:prediction-path}. Thanks to Mikio Braun for helping me to write this short paper.

\bibliography{iclr2020_conference}
\bibliographystyle{iclr2020_conference}

\end{document}
